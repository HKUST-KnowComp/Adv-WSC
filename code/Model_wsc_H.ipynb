{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"g0TwFhv4RPuh"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import json\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKpZiiP3R5ka"},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBPTer8cR-t6"},"outputs":[],"source":["import openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dr3bK1wu_F9t"},"outputs":[],"source":["from openai import OpenAI\n","prompt=\"hh\"\n","client = OpenAI(\n","    api_key=\"\",\n","    base_url=\"https://api.aimlapi.com\",\n",")\n","\n","response = client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n",")\n","\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZKfQ_rFVSdy"},"outputs":[],"source":["dataset = pd.read_csv(\"/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/wsc_273_annotated_final.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fq77H9Ry4wd8","scrolled":true},"outputs":[],"source":["import time\n","gens = []\n","answers = []\n","cnt = 0\n","text_list=[]\n","for i in range(0, len(dataset)-1, 2):\n","    if i >= 254:\n","        i += 1\n","    text1 = dataset['text'][i]\n","    text2 = dataset['text'][i+1]\n","    pronoun = dataset['pronoun'][i]\n","\n","    ori_ans1, ori_ans2 = json.loads(dataset[\"options\"][i])\n","    if isinstance(dataset[\"mod\"][i], str):\n","        ans = json.loads(dataset[\"mod\"][i])\n","        ans1, ans2 = ans[0], ans[1]\n","    else:\n","        continue\n","\n","    if isinstance(dataset[\"mod_text\"][i], str):\n","        text1 = dataset[\"mod_text\"][i]\n","        text2 = dataset[\"mod_text\"][i+1]\n","    else:\n","        text1 = text1.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","        text2 = text2.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","\n","    def get_question(pronoun):\n","        pronoun = pronoun.lower()\n","        if pronoun in [\"they\", \"them\"]:\n","            return f\" What do \\\"{pronoun}\\\" refer to?\"\n","        if pronoun in [\"he\", \"she\", \"it\", \"her\", \"him\", \"his\"]:\n","            return f\" What does \\\"{pronoun}\\\" refer to?\"\n","    text_list.append(text1)\n","    text_list.append(text2)\n","    prompt = f\"Q: Compare the two sentences and answer the questions\\n1. {text1 + get_question(pronoun)}\\n2. {text2 + get_question(pronoun)}\\nSelect from [\\\"{ans1}\\\", \\\"{ans2}\\\"]\\nAnswer in the format that 1.Answer for the first question 2. 1.Answer for the second question\\nA:\"\n","    gens.append(\n","        client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n","    )\n","    )\n","\n","    answers.append([ans1, ans2])\n","\n","    print(\"=\"*20)\n","    print(i, \"\\n\")\n","    print(prompt)\n","    print(gens)\n","    cnt += 1\n","    # if(i>5):\n","    #   break\n","\n","print(cnt)\n","\n","df = pd.DataFrame(text_list,columns=['text'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_mis.csv', index=False)\n","results = [gen.choices[0].message.content.strip() for gen in gens]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_Bij3uF4wd9"},"outputs":[],"source":["# import math\n","# if math.isnan(dataset[\"mod\"][i]):\n","#     print(1)\n","df = pd.DataFrame(results,columns=['text'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_result_mis_answer.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWHFSuZ24wd9"},"outputs":[],"source":["[res.split(\"\\n\") for res in  results]"]},{"cell_type":"code","source":["for res in  results:\n","  print(res)\n","  print(\"=\"*20)"],"metadata":{"id":"C226OzXLb1Yf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fyB3rLS4wd-"},"outputs":[],"source":["\n","answer = 'In the first sentence, \"he\" refers to Tom. In the second sentence, \"he\" refers to Ray.'\n","gold = [\"Tom\", \"Ray\"]\n","\n","def loosely_match_results(a_, ans_):\n","    if a_.lower() in ans_[0].lower():\n","        return 0\n","    elif a_.lower().replace(\"the\", \"\").strip() in ans_[0].lower():\n","        return 0\n","    elif a_.lower().replace(\"a\", \"\").strip() in ans_[0].lower():\n","        return 0\n","    elif a_.lower() in ans_[1].lower():\n","        return 1\n","    elif a_.lower().replace(\"the\", \"\").strip() in ans_[1].lower():\n","        return 1\n","    elif a_.lower().replace(\"a\", \"\").strip() in ans_[1].lower():\n","        return 1\n","    else:\n","        return -1\n","\n","\n","def parse_answer_1(pred):\n","    # print(pred)\n","    if \"1.\" not in pred:\n","        # print(pred)\n","        print(\"not follow the instruction\")\n","        return \"\",\"\"\n","    if \"2.\" not in pred:\n","        # print(pred)\n","        print(\"not follow the instruction\")\n","        return \"\",\"\"\n","    pred=pred[pred.rfind(\"1.\"):].strip()\n","\n","\n","    ans1, ans2 = pred.split(\"2. \")\n","    ans1, ans2 =ans1.split(\"\\n\")[0], ans2.split(\"\\n\")[0]\n","    ans1, ans2 = ans1.strip(), ans2.strip()\n","    ans1 = ans1.replace(\"1.\", \"\")\n","    if \"refers to\" in ans1:\n","        idx = ans1.find(\"refers to\") + len(\"refers to\")\n","        ans1 = ans1[idx:].replace(\".\", \"\")\n","    elif \"refer to\" in ans1:\n","        idx = ans1.find(\"refer to\") + len(\"refer to\")\n","        ans1 = ans1[idx:].replace(\".\", \"\")\n","    else:\n","        ans1 = ans1\n","\n","\n","\n","    ans2 = ans2.replace(\"2.\", \"\")\n","    if \"refers to\" in ans2:\n","        idx = ans2.find(\"refers to\") + len(\"refers to\")\n","        ans2 = ans2[idx:].replace(\".\", \"\")\n","    elif \"refer to\" in ans2:\n","        idx = ans2.find(\"refer to\") + len(\"refer to\")\n","        ans2 = ans2[idx:].replace(\".\", \"\")\n","    else:\n","        ans2 = ans2\n","\n","    return ans1.replace('\"', '').strip(), ans2.replace('\"', '').strip()\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"5DNGfGqCaU0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbLouRsKpvSU"},"outputs":[],"source":["parsed_results = []\n","em = 0\n","correct_cnt = 0\n","predictions = []\n","for result, answer in zip(results, answers):\n","    print(\"*\"*20)\n","\n","    a1, a2 = parse_answer_1(result)\n","    pred_0 = loosely_match_results(a1, answer)\n","    pred_1 = loosely_match_results(a2, answer)\n","\n","    if pred_0 == 0 and pred_1 == 1:\n","        em += 2\n","    correct_cnt += int((pred_0 == 0)) + int((pred_1 == 1))\n","    if pred_0 == -1 or pred_1 == -1:\n","        print(result)\n","        print(\"wrong parsing\", a1, a2, answer)\n","    predictions.append(int(pred_0 == 0))\n","    predictions.append(int(pred_1 == 1))\n","#     print(pred_0 == 0 and pred_1 == 1, \"|\", a1, \"|\", a2, \"|\", answer)\n","print(\"em\", em/len(results)/2, \"acc\", correct_cnt/len(results)/2)\n","\n","df = pd.DataFrame(predictions,columns=['result'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_result_mis.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"z_z8HECfVPNv"},"source":["# one-shot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24ue0mO2VPNv"},"outputs":[],"source":["import time\n","gens_oneshot = []\n","answers = []\n","cnt = 0\n","for i in range(0, len(dataset)-1, 2):\n","    if i >= 254:\n","        i += 1\n","    text1 = dataset['text'][i]\n","    text2 = dataset['text'][i+1]\n","    pronoun = dataset['pronoun'][i]\n","    ori_ans1, ori_ans2 = json.loads(dataset[\"options\"][i])\n","    if isinstance(dataset[\"mod\"][i], str):\n","        ans = json.loads(dataset[\"mod\"][i])\n","        ans1, ans2 = ans[0], ans[1]\n","    else:\n","        continue\n","\n","    if isinstance(dataset[\"mod_text\"][i], str):\n","        text1 = dataset[\"mod_text\"][i]\n","        text2 = dataset[\"mod_text\"][i+1]\n","    else:\n","        text1 = text1.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","        text2 = text2.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","\n","    def get_question(pronoun):\n","        pronoun = pronoun.lower()\n","        if pronoun in [\"they\", \"them\"]:\n","            return f\" What do \\\"{pronoun}\\\" refer to?\"\n","        if pronoun in [\"he\", \"she\", \"it\", \"her\", \"him\", \"his\"]:\n","            return f\" What does \\\"{pronoun}\\\" refer to?\"\n","\n","    examplar = \"Q: Compare the two sentences and answer the questions\\n1. The fish ate the worm. It was hungry. What does \\\"it\\\" refer to?\\n2. The fish ate the worm. It was tasty. What does \\\"it\\\" refer to?\\nSelect from [\\\"The fish\\\", \\\"The worm\\\"]\\nA: 1. The fish. 2. The worm\"\n","\n","    prompt = examplar + \"\\n\\n\" + f\"Q: Compare the two sentences and answer the questions\\n1. {text1 + get_question(pronoun)}\\n2. {text2 + get_question(pronoun)}\\nSelect from [\\\"{ans1}\\\", \\\"{ans2}\\\"]\\nA: \"\n","\n","    gens_oneshot.append(\n","        client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n","    )\n","        )\n","    answers.append([ans1, ans2])\n","    time.sleep(0.1)\n","#     print(\"=\"*20)\n","#     print(i, \"\\n\")\n","#     print(prompt)\n","    cnt += 1\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xsl1AznsfGUp"},"outputs":[],"source":["results_oneshot = [gen.choices[0].message.content.strip() for gen in gens_oneshot]\n","print(results_oneshot)\n","df = pd.DataFrame(results_oneshot,columns=['text'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_result_one_shot_mis_answer.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caSIOQ5-VPNw"},"outputs":[],"source":["def parse_answer_oneshot(pred):\n","    if \"1.\" not in pred:\n","        print(pred)\n","        print(\"not follow the instruction\")\n","        return \"\",\"\"\n","    if \"2.\" not in pred:\n","        print(pred)\n","        print(\"not follow the instruction\")\n","        return \"\",\"\"\n","    pred=pred[pred.rfind(\"1.\"):].strip()\n","\n","\n","    ans1, ans2 = pred.split(\"2. \")\n","    ans1, ans2 =ans1.split(\"\\n\")[0], ans2.split(\"\\n\")[0]\n","    ans1, ans2 =ans1.split(\"(\")[0], ans2.split(\"(\")[0]\n","    ans1, ans2 = ans1.strip(), ans2.strip()\n","    ans1 = ans1.replace(\"1.\", \"\")\n","    if \"refers to\" in ans1:\n","        idx = ans1.find(\"refers to\") + len(\"refers to\")\n","        ans1 = ans1[idx:].replace(\".\", \"\")\n","    elif \"refer to\" in ans1:\n","        idx = ans1.find(\"refer to\") + len(\"refer to\")\n","        ans1 = ans1[idx:].replace(\".\", \"\")\n","    else:\n","        ans1 = ans1\n","\n","\n","\n","    ans2 = ans2.replace(\"2.\", \"\")\n","    if \"refers to\" in ans2:\n","        idx = ans2.find(\"refers to\") + len(\"refers to\")\n","        ans2 = ans2[idx:].replace(\".\", \"\")\n","    elif \"refer to\" in ans2:\n","        idx = ans2.find(\"refer to\") + len(\"refer to\")\n","        ans2 = ans2[idx:].replace(\".\", \"\")\n","    else:\n","        ans2 = ans2\n","\n","    if ans1.endswith(\".\"):\n","        ans1 = ans1.replace(\".\", \"\")\n","    if ans2.endswith(\".\"):\n","        ans2 = ans2.replace(\".\", \"\")\n","    return ans1.replace('\"', '').strip(), ans2.replace('\"', '').strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_e9ydw2hVPNw"},"outputs":[],"source":["# oneshot: fish and worm case\n","parsed_results = []\n","em = 0\n","correct_cnt = 0\n","predictions = []\n","for result, answer in zip(results_oneshot, answers):\n","    print(\"*\"*20)\n","    a1, a2 = parse_answer_oneshot(result)\n","    pred_0 = loosely_match_results(a1, answer)\n","    pred_1 = loosely_match_results(a2, answer)\n","    if pred_0 == 0 and pred_1 == 1:\n","        em += 2\n","    correct_cnt += int((pred_0 == 0)) + int((pred_1 == 1))\n","    # if pred_0 == -1 or pred_1 == -1:\n","    #     print(result)\n","    #     print(\"wrong parsing\", a1, a2)\n","    #     print(answer)\n","    predictions.append(int(pred_0 == 0))\n","    predictions.append(int(pred_1 == 1))\n","\n","df = pd.DataFrame(predictions,columns=['result'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_one_shot_result_mis.csv', index=False)\n","print(\"em\", em/len(results)/2, \"acc\", correct_cnt/len(results)/2)"]},{"cell_type":"markdown","metadata":{"id":"uD5kZP8Kzz0M"},"source":["Cot-one shot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVwKgqFbzzBT"},"outputs":[],"source":["import time\n","gens_oneshot = []\n","answers = []\n","cnt = 0\n","for i in range(0, len(dataset)-1, 2):\n","    if i >= 254:\n","        i += 1\n","    text1 = dataset['text'][i]\n","    text2 = dataset['text'][i+1]\n","    pronoun = dataset['pronoun'][i]\n","    ori_ans1, ori_ans2 = json.loads(dataset[\"options\"][i])\n","    if isinstance(dataset[\"mod\"][i], str):\n","        ans = json.loads(dataset[\"mod\"][i])\n","        ans1, ans2 = ans[0], ans[1]\n","    else:\n","        continue\n","\n","    if isinstance(dataset[\"mod_text\"][i], str):\n","        text1 = dataset[\"mod_text\"][i]\n","        text2 = dataset[\"mod_text\"][i+1]\n","    else:\n","        text1 = text1.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","        text2 = text2.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","\n","    def get_question(pronoun):\n","        pronoun = pronoun.lower()\n","        if pronoun in [\"they\", \"them\"]:\n","            return f\" What do \\\"{pronoun}\\\" refer to?\"\n","        if pronoun in [\"he\", \"she\", \"it\", \"her\", \"him\", \"his\"]:\n","            return f\" What does \\\"{pronoun}\\\" refer to?\"\n","\n","    examplar = \"Q: Compare the two sentences and answer the questions\\n\\\n","1. The fish ate the worm, it was tasty. What does \\\"it\\\" refer to?\\n\\\n","2. The fish ate the worm, it was hungry. What does \\\"it\\\" refer to?\\n\\\n","Select from [\\\"fish\\\", \\\"worm\\\"]\\n\\n\\\n","In the first sentence, the worm is the main object that was eaten, the one that is eaten should be considered as tasty. In the second sentence, the fish was the one eating so it must be hungry. Thus the answer is:\\n\\\n","A: 1. worm 2. fish\"\n","\n","    prompt = examplar + \"\\n\\n\" + f\"Q: Compare the two sentences and answer the questions\\n1. {text1 + get_question(pronoun)}\\n2. {text2 + get_question(pronoun)}\\nSelect from [\\\"{ans1}\\\", \\\"{ans2}\\\"]\\nA: \"\n","    # gens_oneshot.append(openai.Completion.create(\n","    #               model=\"text-davinci-003\",\n","    #               prompt=prompt,\n","    #               max_tokens=256,\n","    #               temperature=0\n","    # ))\n","    gens_oneshot.append(\n","        client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n","    )\n","        )\n","    answers.append([ans1, ans2])\n","    time.sleep(0.1)\n","    print(\"=\"*20)\n","    print(i, \"\\n\")\n","    print(prompt)\n","    cnt += 1\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOSbpXhXz3-6"},"outputs":[],"source":["results_oneshot = [gen.choices[0].message.content.strip() for gen in gens_oneshot]\n","print(results_oneshot)\n","df = pd.DataFrame(results_oneshot,columns=['text'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_result_cot_one_shot_mis_answer.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKsZM5nnz4IM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTDV7ducz92w"},"outputs":[],"source":["# oneshot: fish and worm case\n","parsed_results = []\n","em = 0\n","correct_cnt = 0\n","predictions = []\n","for result, answer in zip(results_oneshot, answers):\n","    a1, a2 = parse_answer_oneshot(result)\n","    pred_0 = loosely_match_results(a1, answer)\n","    pred_1 = loosely_match_results(a2, answer)\n","    if pred_0 == 0 and pred_1 == 1:\n","        em += 2\n","    correct_cnt += int((pred_0 == 0)) + int((pred_1 == 1))\n","    if pred_0 == -1 or pred_1 == -1:\n","        print(\"*\"*100)\n","        print(result)\n","        print(\"wrong parsing\", a1, a2, answer)\n","    predictions.append(int(pred_0 == 0))\n","    predictions.append(int(pred_1 == 1))\n","#     print(pred_0 == 0 and pred_1 == 1, \"|\", a1, \"|\", a2, \"|\", answer)\n","df = pd.DataFrame(predictions,columns=['result'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_cot_one_shot_result_mis.csv', index=False)\n","print(\"em\", em/len(results_oneshot)/2, \"acc\", correct_cnt/len(results_oneshot)/2)"]},{"cell_type":"markdown","metadata":{"id":"cnbKHqvY44p3"},"source":["CoT-Let's think step by step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSKfs-WB43kD"},"outputs":[],"source":["import time\n","gens_oneshot = []\n","answers = []\n","cnt = 0\n","for i in range(0, len(dataset)-1, 2):\n","    if i >= 254:\n","        i += 1\n","    text1 = dataset['text'][i]\n","    text2 = dataset['text'][i+1]\n","    pronoun = dataset['pronoun'][i]\n","    ori_ans1, ori_ans2 = json.loads(dataset[\"options\"][i])\n","    if isinstance(dataset[\"mod\"][i], str):\n","        ans = json.loads(dataset[\"mod\"][i])\n","        ans1, ans2 = ans[0], ans[1]\n","    else:\n","        continue\n","\n","    if isinstance(dataset[\"mod_text\"][i], str):\n","        text1 = dataset[\"mod_text\"][i]\n","        text2 = dataset[\"mod_text\"][i+1]\n","    else:\n","        text1 = text1.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","        text2 = text2.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","\n","    def get_question(pronoun):\n","        pronoun = pronoun.lower()\n","        if pronoun in [\"they\", \"them\"]:\n","            return f\" What do \\\"{pronoun}\\\" refer to?\"\n","        if pronoun in [\"he\", \"she\", \"it\", \"her\", \"him\", \"his\"]:\n","            return f\" What does \\\"{pronoun}\\\" refer to?\"\n","\n","    # examplar = \"Q: Compare the two sentences and answer the questions\\n1. The fish ate the worm. It was hungry. What does \\\"it\\\" refer to?\\n2. The fish ate the worm. It was tasty. What does \\\"it\\\" refer to?\\nSelect from [\\\"The fish\\\", \\\"The worm\\\"]\\nA: 1. The fish. 2. The worm\"\n","\n","    prompt =  f\"Q: Compare the two sentences and answer the questions\\n1. {text1 + get_question(pronoun)}\\n2. {text2 + get_question(pronoun)}\\nSelect from [\\\"{ans1}\\\", \\\"{ans2}\\\"]\\nAnswer in the format that 1.Answer for the first question 2. 1.Answer for the second question\\nlet's think step by step\\nA: \"\n","    # gens_oneshot.append(openai.Completion.create(\n","    #               model=\"text-davinci-003\",\n","    #               prompt=prompt,\n","    #               max_tokens=256,\n","    #               temperature=0\n","    # ))\n","    gens_oneshot.append(\n","        client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n","    )\n","        )\n","    answers.append([ans1, ans2])\n","    time.sleep(0.1)\n","#     print(\"=\"*20)\n","#     print(i, \"\\n\")\n","#     print(prompt)\n","    cnt += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0kb1VKuU4ov"},"outputs":[],"source":["results_oneshot = [gen.choices[0].message.content.strip() for gen in gens_oneshot]\n","print(results_oneshot)\n","df = pd.DataFrame(results_oneshot,columns=['text'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_result_cot_mis_answer.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1wPmYM1435U"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAJjRrR044CE"},"outputs":[],"source":["# oneshot: fish and worm case\n","parsed_results = []\n","em = 0\n","correct_cnt = 0\n","predictions = []\n","for result, answer in zip(results_oneshot, answers):\n","    print(\"*\"*20)\n","    a1, a2 = parse_answer_oneshot(result)\n","    pred_0 = loosely_match_results(a1, answer)\n","    pred_1 = loosely_match_results(a2, answer)\n","    if pred_0 == 0 and pred_1 == 1:\n","        em += 2\n","    correct_cnt += int((pred_0 == 0)) + int((pred_1 == 1))\n","    # if pred_0 == -1 or pred_1 == -1:\n","    #     print(result)\n","    #     print(\"wrong parsing\")\n","    predictions.append(int(pred_0 == 0))\n","    predictions.append(int(pred_1 == 1))\n","\n","df = pd.DataFrame(predictions,columns=['result'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_step_result_mis.csv', index=False)\n","print(\"em\", em/len(results_oneshot)/2, \"acc\", correct_cnt/len(results_oneshot)/2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jr4BIgKn5Fey"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdTkTJjL5Fml"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyXKgRJz5FvF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"RBoWfiH2VPNx"},"source":["# CoT conceptualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kzbja_H8VPNx"},"outputs":[],"source":["import time\n","from tqdm import tqdm\n","cot_concept_prompt = \"Q: Compare the two sentences and answer the questions\\n\\\n","1. The tasty fish ate the worm, it was tasty. What does \\\"it\\\" refer to?\\n\\\n","2. The tasty fish ate the worm, it was hungry. What does \\\"it\\\" refer to?\\n\\\n","Select from [\\\"tasty fish\\\", \\\"worm\\\"]\\n\\n\\\n","Conceptualization: Fish can be conceptualized as a predator, and worm can be conceptualized as a prey.\\n\\\n","The question can be conceptualized as:\\n\\\n","1. The predator ate the prey, it was tasty. What does \\\"it\\\" refer to?\\n\\\n","2. The predator ate the prey, it was hungry. What does \\\"it\\\" refer to?\\n\\\n","Select from [\\\"prey\\\", \\\"predator\\\"]\\n\\\n","Because the subject of \\\"ate\\\" should be hungry and the object should be tasty, so:\\n\\\n","Answer: 1. prey. 2. predator\\n\\n\\\n","Conclusion: As worm is a prey, and fish is a predator in the context,\\n\\\n","A: Thus the answer is:\\n\\\n","1. worm 2. fish\"\n","\n","gens_cot_concept = []\n","answers = []\n","cnt = 0\n","for i in tqdm(range(0, len(dataset)-1, 2)):\n","    if i >= 254:\n","        i += 1\n","    text1 = dataset['text'][i]\n","    text2 = dataset['text'][i+1]\n","    pronoun = dataset['pronoun'][i]\n","    ori_ans1, ori_ans2 = json.loads(dataset[\"options\"][i])\n","    if isinstance(dataset[\"mod\"][i], str):\n","        ans = json.loads(dataset[\"mod\"][i])\n","        ans1, ans2 = ans[0], ans[1]\n","    else:\n","        continue\n","\n","    if isinstance(dataset[\"mod_text\"][i], str):\n","        text1 = dataset[\"mod_text\"][i]\n","        text2 = dataset[\"mod_text\"][i+1]\n","    else:\n","        text1 = text1.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","        text2 = text2.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","\n","    def get_question(pronoun):\n","        pronoun = pronoun.lower()\n","        if pronoun in [\"they\", \"them\"]:\n","            return f\" What do \\\"{pronoun}\\\" refer to?\"\n","        if pronoun in [\"he\", \"she\", \"it\", \"her\", \"him\", \"his\"]:\n","            return f\" What does \\\"{pronoun}\\\" refer to?\"\n","\n","    prompt = cot_concept_prompt + \"\\n\\n\" + f\"Q: Compare the two sentences and answer the questions\\n1. {text1 + get_question(pronoun)}\\n2. {text2 + get_question(pronoun)}\\nSelect from [\\\"{ans1}\\\", \\\"{ans2}\\\"]\\nA: \"\n","    gens_cot_concept.append(\n","      client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n","      )\n","    )\n","    answers.append([ans1, ans2])\n","    time.sleep(0.1)\n","#     print(prompt)\n","    cnt += 1\n","\n","\n","results_cot_concept_direct = [gen.choices[0].message.content.strip() for gen in gens_cot_concept]\n","print(results_cot_concept_direct)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRwI1-vuaBZj"},"outputs":[],"source":["df = pd.DataFrame(results,columns=['text'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_result_AoT_mis_answer.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZ1IdkDhVPNx"},"outputs":[],"source":["# cot: direct prompt answer\n","parsed_results = []\n","em = 0\n","correct_cnt = 0\n","predictions = []\n","for result, answer in zip(results_cot_concept_direct, answers):\n","    print(\"*\"*10)\n","    a1, a2 = parse_answer_oneshot(result)\n","    pred_0 = loosely_match_results(a1, answer)\n","    pred_1 = loosely_match_results(a2, answer)\n","    if pred_0 == 0 and pred_1 == 1:\n","        em += 2\n","    correct_cnt += int((pred_0 == 0)) + int((pred_1 == 1))\n","    if pred_0 == -1 or pred_1 == -1:\n","        print(result)\n","        print(\"wrong parsing\", a1, a2, answer)\n","    predictions.append(int(pred_0 == 0))\n","    predictions.append(int(pred_1 == 1))\n","#     print(pred_0 == 0 and pred_1 == 1, \"|\", a1, \"|\", a2, \"|\", answer)\n","df = pd.DataFrame(predictions,columns=['result'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_AoT_result_mis.csv', index=False)\n","print(\"em\", em/len(results)/2, \"acc\", correct_cnt/len(results)/2)"]},{"cell_type":"markdown","metadata":{"id":"pYzxYdDjVPN1"},"source":["# WinoWhy style exampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uQd_b1QVPN1"},"outputs":[],"source":["# winowhy\n","\n","cot_winowhy_prompt = \"Q: Compare the two sentences and answer the questions\\n\\\n","1. The tasty fish ate the worm, it was tasty. What does \\\"it\\\" refer to?\\n\\\n","2. The tasty fish ate the worm, it was hungry. What does \\\"it\\\" refer to?\\n\\\n","Select from [\\\"tasty fish\\\", \\\"worm\\\"]\\n\\n\\\n","In the first sentence, the worm is the main object that the very (ate) refers to so the answer is worm. In the second sentence, the fish was the one eating so it must be hungry. Thus the answer is:\\n\\\n","A: 1. worm 2. fish\"\n","\n","### ask to generate all conceptualization\n","\n","import time\n","from tqdm import tqdm\n","\n","\n","gens_winowhy = []\n","answers = []\n","cnt = 0\n","for i in tqdm(range(0, len(dataset)-1, 2)):\n","    if i >= 254:\n","        i += 1\n","    text1 = dataset['text'][i]\n","    text2 = dataset['text'][i+1]\n","    pronoun = dataset['pronoun'][i]\n","    ori_ans1, ori_ans2 = json.loads(dataset[\"options\"][i])\n","    if isinstance(dataset[\"mod\"][i], str):\n","        ans = json.loads(dataset[\"mod\"][i])\n","        ans1, ans2 = ans[0], ans[1]\n","    else:\n","        continue\n","\n","    if isinstance(dataset[\"mod_text\"][i], str):\n","        text1 = dataset[\"mod_text\"][i]\n","        text2 = dataset[\"mod_text\"][i+1]\n","    else:\n","        text1 = text1.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","        text2 = text2.lower().replace(ori_ans1.lower(), ans1).replace(ori_ans2.lower(), ans2)\n","\n","    def get_question(pronoun):\n","        pronoun = pronoun.lower()\n","        if pronoun in [\"they\", \"them\"]:\n","            return f\" What do \\\"{pronoun}\\\" refer to?\"\n","        if pronoun in [\"he\", \"she\", \"it\", \"her\", \"him\", \"his\"]:\n","            return f\" What does \\\"{pronoun}\\\" refer to?\"\n","\n","    prompt = cot_winowhy_prompt + \"\\n\\n\" + f\"Q: Compare the two sentences and answer the questions\\n1. {text1 + get_question(pronoun)}\\n2. {text2 + get_question(pronoun)}\\nSelect from [\\\"{ans1}\\\", \\\"{ans2}\\\"]\\n\"\n","    gens_winowhy.append(\n","        client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt,\n","        },\n","    ],\n","    )\n","        )\n","    answers.append([ans1, ans2])\n","\n","    cnt += 1\n","\n","results_winowhy = [gen.choices[0].message.content.strip() for gen in gens_winowhy]\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qACUHOfVPN1","scrolled":true},"outputs":[],"source":["# cot: direct prompt answer\n","parsed_results = []\n","em = 0\n","correct_cnt = 0\n","predictions = []\n","for result, answer in zip(results_winowhy, answers):\n","    if result.startswith(\"A:\"):\n","        result = result[2:]\n","    if result.find(\"\\nA:\") != -1:\n","        result = result[result.find(\"\\nA:\")+len(\"\\nA:\"):]\n","\n","    print(result)\n","    a1, a2 = parse_answer_oneshot(result)\n","    pred_0 = loosely_match_results(a1, answer)\n","    pred_1 = loosely_match_results(a2, answer)\n","    if pred_0 == 0 and pred_1 == 1:\n","        em += 2\n","    correct_cnt += int((pred_0 == 0)) + int((pred_1 == 1))\n","    if pred_0 == -1 or pred_1 == -1:\n","        print(\"wrong parsing\", a1, a2, answer)\n","    predictions.append(int(pred_0 == 0))\n","    predictions.append(int(pred_1 == 1))\n","#     print(pred_0 == 0 and pred_1 == 1, \"|\", a1, \"|\", a2, \"|\", answer)\n","df = pd.DataFrame(predictions,columns=['result'])\n","df.to_csv(f'/content/gdrive/MyDrive/long-tail_knowledge/wsc-test-master/text_origin_tested_win_result_mis.csv', index=False)\n","print(\"em\", em/len(results_winowhy)/2, \"acc\", correct_cnt/len(results_winowhy)/2)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}